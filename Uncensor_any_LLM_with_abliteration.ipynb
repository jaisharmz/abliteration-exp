{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82acAhWYGIPx"
      },
      "source": [
        "# Uncensor any LLM with abliteration\n",
        "\n",
        "> üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BErEJu5WVekL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "roboflow 1.1.21 requires opencv-python-headless==4.8.0.74, but you have opencv-python-headless 4.5.5.64 which is incompatible.\n",
            "salesforce-lavis 1.0.2 requires transformers<4.27,>=4.25.0, but you have transformers 4.55.4 which is incompatible.\n",
            "torchaudio 2.2.2+cu118 requires torch==2.2.2+cu118, but you have torch 2.8.0 which is incompatible.\n",
            "torchvision 0.17.2+cu118 requires torch==2.2.2+cu118, but you have torch 2.8.0 which is incompatible.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "Could not import module 'BertForPreTraining'. Are this object's requirements defined correctly?",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2292\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2292\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2293\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2322\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2320\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:34\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_layers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientCheckpointingLayer\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     37\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     45\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_layers.py:28\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Unpack\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformersKwargs, auto_docstring, can_return_tuple, logging\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\processing_utils.py:39\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, is_vision_available, load_image\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m render_jinja_template\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_utils.py:59\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[0;32m     61\u001b[0m     pil_torch_interpolation_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     62\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mNEAREST: InterpolationMode\u001b[38;5;241m.\u001b[39mNEAREST_EXACT,\n\u001b[0;32m     63\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mBOX: InterpolationMode\u001b[38;5;241m.\u001b[39mBOX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mLANCZOS: InterpolationMode\u001b[38;5;241m.\u001b[39mLANCZOS,\n\u001b[0;32m     68\u001b[0m     }\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\_meta_registrations.py:163\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_custom_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision::nms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\library.py:1069\u001b[0m, in \u001b[0;36mregister_fake.<locals>.register\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m   1068\u001b[0m     use_lib \u001b[38;5;241m=\u001b[39m lib\n\u001b[1;32m-> 1069\u001b[0m \u001b[43muse_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_override\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\library.py:219\u001b[0m, in \u001b[0;36mLibrary._register_fake\u001b[1;34m(self, op_name, fn, _stacklevel, allow_override)\u001b[0m\n\u001b[0;32m    217\u001b[0m     func_to_register \u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m--> 219\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_to_register\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_override\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registration_handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_library\\fake_impl.py:50\u001b[0m, in \u001b[0;36mFakeImplHolder.register\u001b[1;34m(self, func, source, lib, allow_override)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an fake impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m     )\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m     )\n",
            "\u001b[1;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer, utils\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhook_points\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookPoint\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformer_lens\\__init__.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFactoredMatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactoredMatrix\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mActivationCache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActivationCache\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHookedTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSVDInterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVDInterpreter\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHookedEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedEncoder\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformer_lens\\HookedTransformer.py:43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerBase\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading_from_pretrained\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mloading\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mActivationCache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActivationCache\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformer_lens\\loading_from_pretrained.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     AutoConfig,\n\u001b[0;32m     19\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m     20\u001b[0m     BertForPreTraining,\n\u001b[0;32m     21\u001b[0m     T5ForConditionalGeneration,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHookedTransformerConfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformerConfig\n",
            "File \u001b[1;32mc:\\Users\\Rithwik Nukala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2295\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2293\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2294\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m   2296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Are this object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms requirements defined correctly?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2297\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   2300\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: Could not import module 'BertForPreTraining'. Are this object's requirements defined correctly?"
          ]
        }
      ],
      "source": [
        "!pip install -qqq transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping --progress-bar off\n",
        "\n",
        "import torch\n",
        "import functools\n",
        "import einops\n",
        "import gc\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "from typing import List\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from jaxtyping import Float, Int\n",
        "from collections import defaultdict\n",
        "\n",
        "# Turn automatic differentiation off to save GPU memory (credit: Undi95)\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "def reformat_texts(texts):\n",
        "    return [[{\"role\": \"user\", \"content\": text}] for text in texts]\n",
        "\n",
        "# Get harmful and harmless datasets\n",
        "def get_harmful_instructions():\n",
        "    dataset = load_dataset('mlabonne/harmful_behaviors')\n",
        "    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n",
        "\n",
        "def get_harmless_instructions():\n",
        "    dataset = load_dataset('mlabonne/harmless_alpaca')\n",
        "    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n",
        "\n",
        "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
        "harmless_inst_train, harmless_inst_test = get_harmless_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O63-4qZw8nfY"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"mlabonne/Daredevil-8B\"\n",
        "NEW_MODEL_ID = \"mlabonne/Daredevil-8B-abliterated\"\n",
        "MODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Download and load model\n",
        "!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = HookedTransformer.from_pretrained_no_processing(\n",
        "    MODEL_TYPE,\n",
        "    local_files_only=True,\n",
        "    dtype=torch.bfloat16,\n",
        "    default_padding_side='left'\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLePKlGnKVU1"
      },
      "outputs": [],
      "source": [
        "def tokenize_instructions(tokenizer, instructions):\n",
        "    return tokenizer.apply_chat_template(\n",
        "        instructions,\n",
        "        padding=True,\n",
        "        truncation=False,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        add_generation_prompt=True,\n",
        "    ).input_ids\n",
        "\n",
        "n_inst_train = min(256, len(harmful_inst_train), len(harmless_inst_train))\n",
        "\n",
        "# Tokenize datasets\n",
        "harmful_tokens = tokenize_instructions(\n",
        "    tokenizer,\n",
        "    instructions=harmful_inst_train[:n_inst_train],\n",
        ")\n",
        "harmless_tokens = tokenize_instructions(\n",
        "    tokenizer,\n",
        "    instructions=harmless_inst_train[:n_inst_train],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhBNPb95tbmm"
      },
      "outputs": [],
      "source": [
        "# Define batch size based on available VRAM\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize defaultdicts to store activations\n",
        "harmful = defaultdict(list)\n",
        "harmless = defaultdict(list)\n",
        "\n",
        "# Process the training data in batches\n",
        "num_batches = (n_inst_train + batch_size - 1) // batch_size\n",
        "for i in tqdm(range(num_batches)):\n",
        "    print(i)\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min(n_inst_train, start_idx + batch_size)\n",
        "\n",
        "    # Run models on harmful and harmless prompts, cache activations\n",
        "    harmful_logits, harmful_cache = model.run_with_cache(\n",
        "        harmful_tokens[start_idx:end_idx],\n",
        "        names_filter=lambda hook_name: 'resid' in hook_name,\n",
        "        device='cpu',\n",
        "        reset_hooks_end=True\n",
        "    )\n",
        "    harmless_logits, harmless_cache = model.run_with_cache(\n",
        "        harmless_tokens[start_idx:end_idx],\n",
        "        names_filter=lambda hook_name: 'resid' in hook_name,\n",
        "        device='cpu',\n",
        "        reset_hooks_end=True\n",
        "    )\n",
        "\n",
        "    # Collect and store the activations\n",
        "    for key in harmful_cache:\n",
        "        harmful[key].append(harmful_cache[key])\n",
        "        harmless[key].append(harmless_cache[key])\n",
        "\n",
        "    # Flush RAM and VRAM\n",
        "    del harmful_logits, harmless_logits, harmful_cache, harmless_cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Concatenate the cached activations\n",
        "harmful = {k: torch.cat(v) for k, v in harmful.items()}\n",
        "harmless = {k: torch.cat(v) for k, v in harmless.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX4w0HeUwAFm"
      },
      "outputs": [],
      "source": [
        "# Helper function to get activation index\n",
        "def get_act_idx(cache_dict, act_name, layer):\n",
        "    key = (act_name, layer)\n",
        "    return cache_dict[utils.get_act_name(*key)]\n",
        "\n",
        "# Compute difference of means between harmful and harmless activations at intermediate layers\n",
        "activation_layers = [\"resid_pre\", \"resid_mid\", \"resid_post\"]\n",
        "activation_refusals = defaultdict(list)\n",
        "\n",
        "for layer_num in range(1, model.cfg.n_layers):\n",
        "    pos = -1  # Position index\n",
        "\n",
        "    for layer in activation_layers:\n",
        "        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=0)\n",
        "        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        refusal_dir = harmful_mean_act - harmless_mean_act\n",
        "        refusal_dir = refusal_dir / refusal_dir.norm()\n",
        "        activation_refusals[layer].append(refusal_dir)\n",
        "\n",
        "# Get all calculated potential refusal directions, sort them in descending order based on their mean\n",
        "# Use a subset of layers if certain activations are not promising\n",
        "selected_layers = [\"resid_pre\"]\n",
        "\n",
        "activation_scored = sorted(\n",
        "    [\n",
        "        activation_refusals[layer][l - 1]\n",
        "        for l in range(1, model.cfg.n_layers)\n",
        "        for layer in selected_layers\n",
        "    ],\n",
        "    key=lambda x: abs(x.mean()),\n",
        "    reverse=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vhhwl-2-jPg"
      },
      "outputs": [],
      "source": [
        "def _generate_with_hooks(\n",
        "    model: HookedTransformer,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    tokens: Int[Tensor, \"batch_size seq_len\"],\n",
        "    max_tokens_generated: int = 64,\n",
        "    fwd_hooks=[],\n",
        ") -> List[str]:\n",
        "    all_tokens = torch.zeros(\n",
        "        (tokens.shape[0], tokens.shape[1] + max_tokens_generated),\n",
        "        dtype=torch.long,\n",
        "        device=tokens.device,\n",
        "    )\n",
        "    all_tokens[:, : tokens.shape[1]] = tokens\n",
        "    for i in range(max_tokens_generated):\n",
        "        with model.hooks(fwd_hooks=fwd_hooks):\n",
        "            logits = model(all_tokens[:, : -max_tokens_generated + i])\n",
        "            next_tokens = logits[:, -1, :].argmax(\n",
        "                dim=-1\n",
        "            )  # greedy sampling (temperature=0)\n",
        "            all_tokens[:, -max_tokens_generated + i] = next_tokens\n",
        "    return tokenizer.batch_decode(\n",
        "        all_tokens[:, tokens.shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "def get_generations(\n",
        "    model: HookedTransformer,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    instructions: List[str],\n",
        "    fwd_hooks=[],\n",
        "    max_tokens_generated: int = 64,\n",
        "    batch_size: int = 4,\n",
        ") -> List[str]:\n",
        "    generations = []\n",
        "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
        "        tokens = tokenize_instructions(\n",
        "            tokenizer, instructions=instructions[i : i + batch_size]\n",
        "        )\n",
        "        generation = _generate_with_hooks(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            tokens,\n",
        "            max_tokens_generated=max_tokens_generated,\n",
        "            fwd_hooks=fwd_hooks,\n",
        "        )\n",
        "        generations.extend(generation)\n",
        "    return generations\n",
        "\n",
        "# Inference-time intervention hook\n",
        "def direction_ablation_hook(\n",
        "    activation: Float[Tensor, \"... d_act\"],\n",
        "    hook: HookPoint,\n",
        "    direction: Float[Tensor, \"d_act\"],\n",
        "):\n",
        "    if activation.device != direction.device:\n",
        "        direction = direction.to(activation.device)\n",
        "    proj = (\n",
        "        einops.einsum(\n",
        "            activation, direction.view(-1, 1), \"... d_act, d_act single -> ... single\"\n",
        "        )\n",
        "        * direction\n",
        "    )\n",
        "    return activation - proj\n",
        "\n",
        "# Testing baseline\n",
        "N_INST_TEST = 4\n",
        "baseline_generations = get_generations(\n",
        "    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]\n",
        ")\n",
        "\n",
        "# Evaluating layers defined earlier (needs human evaluation to determine best layer for refusal inhibition)\n",
        "EVAL_N = 20  # Evaluate how many of the top N potential directions\n",
        "evals = []\n",
        "for refusal_dir in tqdm(activation_scored[:EVAL_N]):\n",
        "    hook_fn = functools.partial(direction_ablation_hook, direction=refusal_dir)\n",
        "    fwd_hooks = [\n",
        "        (utils.get_act_name(act_name, layer), hook_fn)\n",
        "        for layer in list(range(model.cfg.n_layers))\n",
        "        for act_name in activation_layers\n",
        "    ]\n",
        "    intervention_generations = get_generations(\n",
        "        model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=fwd_hooks\n",
        "    )\n",
        "    evals.append(intervention_generations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SbBZ-Un14Yc"
      },
      "outputs": [],
      "source": [
        "# Print generations for human evaluation\n",
        "blacklist = [\"I cannot\", \"I can't\"]\n",
        "for i in range(N_INST_TEST):\n",
        "    print(f\"\\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}\")\n",
        "    print(f\"\\nBASELINE COMPLETION:\\n{baseline_generations[i]}\\033[0m\")\n",
        "    for layer_candidate in range(EVAL_N):\n",
        "        if not any(word in evals[layer_candidate][i] for word in blacklist):\n",
        "            print(f\"\\n---\\n\\nLAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:\")\n",
        "            print(evals[layer_candidate][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY5B3-uP_EjE"
      },
      "outputs": [],
      "source": [
        "def get_orthogonalized_matrix(\n",
        "    matrix: Float[Tensor, \"... d_model\"], vec: Float[Tensor, \"d_model\"]\n",
        ") -> Float[Tensor, \"... d_model\"]:\n",
        "    proj = (\n",
        "        einops.einsum(\n",
        "            matrix, vec.view(-1, 1), \"... d_model, d_model single -> ... single\"\n",
        "        )\n",
        "        * vec\n",
        "    )\n",
        "    return matrix - proj\n",
        "\n",
        "# Select the layer with the highest potential refusal direction\n",
        "LAYER_CANDIDATE = 9\n",
        "refusal_dir = activation_scored[LAYER_CANDIDATE]\n",
        "\n",
        "# Orthogonalize the model's weights\n",
        "if refusal_dir.device != model.W_E.device:\n",
        "    refusal_dir = refusal_dir.to(model.W_E.device)\n",
        "model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)\n",
        "\n",
        "for block in tqdm(model.blocks):\n",
        "    if refusal_dir.device != block.attn.W_O.device:\n",
        "        refusal_dir = refusal_dir.to(block.attn.W_O.device)\n",
        "    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)\n",
        "    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)\n",
        "\n",
        "# Generate text with abliterated model\n",
        "orthogonalized_generations = get_generations(\n",
        "    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]\n",
        ")\n",
        "\n",
        "# Print generations\n",
        "for i in range(N_INST_TEST):\n",
        "    if len(baseline_generations) > i:\n",
        "        print(f\"INSTRUCTION {i}: {harmful_inst_test[i]}\")\n",
        "        print(f\"\\033[92mBASELINE COMPLETION:\\n{baseline_generations[i]}\")\n",
        "    print(f\"\\033[91mINTERVENTION COMPLETION:\\n{evals[LAYER_CANDIDATE][i]}\")\n",
        "    print(f\"\\033[95mORTHOGONALIZED COMPLETION:\\n{orthogonalized_generations[i]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGbVGBi0tP-O"
      },
      "outputs": [],
      "source": [
        "# Convert model back to HF safetensors\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)\n",
        "lm_model = hf_model.model\n",
        "\n",
        "state_dict = model.state_dict()\n",
        "lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[\"embed.W_E\"].cpu())\n",
        "\n",
        "for l in range(model.cfg.n_layers):\n",
        "    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(\n",
        "        einops.rearrange(\n",
        "            state_dict[f\"blocks.{l}.attn.W_O\"], \"n h m->m (n h)\", n=model.cfg.n_heads\n",
        "        ).contiguous()\n",
        "    )\n",
        "    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(\n",
        "        torch.transpose(state_dict[f\"blocks.{l}.mlp.W_out\"], 0, 1).contiguous()\n",
        "    )\n",
        "\n",
        "# Push it to the Hugging Face Hub\n",
        "hf_model.push_to_hub(NEW_MODEL_ID)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
