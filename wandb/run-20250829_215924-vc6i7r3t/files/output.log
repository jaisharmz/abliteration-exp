 76%|███████████████████████████████████████████████████████████████████▉                     | 84/110 [03:13<02:15,  5.22s/it]Traceback (most recent call last):
{'loss': 1.6308, 'grad_norm': 0.5687218308448792, 'learning_rate': 0.00018363636363636366, 'entropy': 1.3672202944755554, 'num_tokens': 35240.0, 'mean_token_accuracy': 0.6793734326958656, 'epoch': 0.98}
{'loss': 1.1786, 'grad_norm': 0.4305104911327362, 'learning_rate': 0.00016545454545454545, 'entropy': 1.1711255714700028, 'num_tokens': 68755.0, 'mean_token_accuracy': 0.7418470511565337, 'epoch': 1.88}
{'loss': 1.0263, 'grad_norm': 0.5086777210235596, 'learning_rate': 0.00014727272727272728, 'entropy': 1.0109987468332857, 'num_tokens': 100488.0, 'mean_token_accuracy': 0.7626614667273857, 'epoch': 2.78}
{'loss': 1.0455, 'grad_norm': 0.460834801197052, 'learning_rate': 0.0001290909090909091, 'entropy': 1.0386176528157414, 'num_tokens': 132989.0, 'mean_token_accuracy': 0.7614651016286902, 'epoch': 3.68}
{'loss': 0.9446, 'grad_norm': 0.5434793829917908, 'learning_rate': 0.00011090909090909092, 'entropy': 0.978256344795227, 'num_tokens': 164585.0, 'mean_token_accuracy': 0.7792211158855541, 'epoch': 4.59}
{'loss': 0.8862, 'grad_norm': 0.6801636219024658, 'learning_rate': 9.272727272727273e-05, 'entropy': 0.9049620064529212, 'num_tokens': 198947.0, 'mean_token_accuracy': 0.7880918882988595, 'epoch': 5.49}
{'loss': 0.8601, 'grad_norm': 0.8505043983459473, 'learning_rate': 7.454545454545455e-05, 'entropy': 0.8967980694126438, 'num_tokens': 230918.0, 'mean_token_accuracy': 0.7956024666090269, 'epoch': 6.39}
{'loss': 0.7851, 'grad_norm': 1.049796223640442, 'learning_rate': 5.636363636363636e-05, 'entropy': 0.8272200806720836, 'num_tokens': 262714.0, 'mean_token_accuracy': 0.8119052326357042, 'epoch': 7.29}
  File "/mnt/c/Users/jais/Documents/abliteration-exp/finetune_model_code_generation.py", line 99, in <module>
    trainer.train()
  File "/home/jais/miniconda3/envs/abliteration/lib/python3.11/site-packages/transformers/trainer.py", line 2238, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/jais/miniconda3/envs/abliteration/lib/python3.11/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jais/miniconda3/envs/abliteration/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 1112, in training_step
    return super().training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jais/miniconda3/envs/abliteration/lib/python3.11/site-packages/transformers/trainer.py", line 3845, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/jais/miniconda3/envs/abliteration/lib/python3.11/site-packages/accelerate/accelerator.py", line 2734, in backward
    loss.backward(**kwargs)
  File "/home/jais/miniconda3/envs/abliteration/lib/python3.11/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/jais/miniconda3/envs/abliteration/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/jais/miniconda3/envs/abliteration/lib/python3.11/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
